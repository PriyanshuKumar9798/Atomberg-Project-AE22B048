#!/usr/bin/env python3
import os
import re
import csv
import time
import requests
import difflib
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import parser as dateparser
from collections import defaultdict
from urllib.parse import urlparse
from transformers import pipeline
QUERIES = [
    "Atomberg smart fan review",
    "Havells smart fan review",
    "Atomberg vs Havells smart fan",
    "Atomberg vs Crompton smart fan",
    "best smart ceiling fan in India",
    "most energy efficient smart fan in India"
]
MAX_RESULTS = 50
COMPETITORS = ["Havells", "Crompton", "Orient Electric", "Usha", "Bajaj", "Superfan"]
OUTPUT_FILE = "google_results_smart_fans.csv"
REQUEST_DELAY = 1.5
SENTIMENT_MODEL = "distilbert-base-uncased-finetuned-sst-2-english"
TOPIC_MODEL = "facebook/bart-large-mnli"
TOPIC_LABELS = ["energy efficiency", "design aesthetics", "price and value", "durability", "noise level", "smart features"]
NEAR_DUP_THRESHOLD = 0.9
API_KEY = os.getenv("GOOGLE_API_KEY")
CX_ID = os.getenv("GOOGLE_CX_ID")
if not API_KEY or not CX_ID:
    raise ValueError("Please set GOOGLE_API_KEY and GOOGLE_CX_ID environment variables.")
print("Loading AI models...")
sentiment_pipeline = pipeline("sentiment-analysis", model=SENTIMENT_MODEL, truncation=True, max_length=512)
topic_pipeline = pipeline("zero-shot-classification", model=TOPIC_MODEL)
print("Models loaded.")
def search_google(query, total_results):
    results = []
    start_index = 1
    while len(results) < total_results:
        num = min(10, total_results - len(results))
        url = "https://www.googleapis.com/customsearch/v1"
        params = {"key": API_KEY, "cx": CX_ID, "q": query, "num": num, "start": start_index}
        resp = requests.get(url, params=params)
        resp.raise_for_status()
        items = resp.json().get("items", [])
        if not items:
            break
        results.extend(items)
        start_index += num
        time.sleep(1)
    return results
def fetch_page_text_and_date(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup(["script", "style", "noscript", "iframe"]):
            tag.extract()
        text = re.sub(r"\s+", " ", soup.get_text(separator=" ", strip=True))
        dt = None
        meta_keys = [
            ('meta', {'property': 'article:published_time'}),
            ('meta', {'name': 'pubdate'}),
            ('meta', {'name': 'publishdate'}),
            ('meta', {'name': 'date'}),
            ('meta', {'itemprop': 'datePublished'}),
            ('meta', {'property': 'og:updated_time'}),
            ('time', {})
        ]
        for tag, attrs in meta_keys:
            for el in soup.find_all(tag, attrs=attrs):
                val = el.get('content') or el.get('datetime') or el.get_text()
                if val:
                    try:
                        dt = dateparser.parse(val, fuzzy=True)
                        break
                    except Exception:
                        continue
            if dt:
                break
        if not dt:
            m = re.search(r'(\d{4}-\d{2}-\d{2})', text)
            if m:
                try:
                    dt = dateparser.parse(m.group(1))
                except Exception:
                    pass
        return text, dt
    except Exception as e:
        return f"[Error fetching: {e}]", None
def detect_mentions(text, brands):
    found = []
    t = text.lower()
    for b in brands:
        if b.lower() in t:
            found.append(b)
    return list(dict.fromkeys(found))
def map_sentiment_to_score(label, score):
    if label.upper().startswith("POS"):
        return float(score)
    else:
        return -float(score)
def recency_decay(publish_dt):
    if not publish_dt:
        return 1.0
    if publish_dt.tzinfo is not None and publish_dt.utcoffset() is not None:
        publish_dt = publish_dt.astimezone(None).replace(tzinfo=None)
    now = datetime.utcnow()
    days = (now - publish_dt).days
    if days < 0:
        days = 0
    return 1 / (1 + 0.05 * days)
def is_near_duplicate(existing, new_url, new_title):
    new_domain = urlparse(new_url).netloc
    for ex in existing:
        ex_domain = urlparse(ex['url']).netloc
        if ex_domain == new_domain:
            if difflib.SequenceMatcher(None, ex['title'].lower(), new_title.lower()).ratio() > NEAR_DUP_THRESHOLD:
                return True
    return False
rows = []
brand_list = ["Atomberg"] + COMPETITORS
total_queries = len(QUERIES)
for q_idx, query in enumerate(QUERIES, start=1):
    print(f"\n[{q_idx}/{total_queries}] Searching: {query}")
    hits = search_google(query, MAX_RESULTS)
    print(f"  Found {len(hits)} results.")
    for r_idx, item in enumerate(hits, start=1):
        url = item.get("link", "")
        title = item.get("title", "")
        if any(r['url'] == url for r in rows):
            continue
        if is_near_duplicate(rows, url, title):
            continue
        print(f"    ({r_idx}/{MAX_RESULTS}) Fetching:", url)
        page_text, publish_dt = fetch_page_text_and_date(url)
        mentions = detect_mentions(page_text, brand_list)
        if isinstance(page_text, str) and page_text.strip():
            trunc = page_text[:1000]
            try:
                sres = sentiment_pipeline(trunc, truncation=True, max_length=512)[0]
                s_label = sres.get("label", "")
                s_score = map_sentiment_to_score(s_label, sres.get("score", 0.0))
            except Exception as e:
                print(f"[Sentiment error: {e}]")
                s_label, s_score = "NEUTRAL", 0.0
            try:
                tres = topic_pipeline(trunc, candidate_labels=TOPIC_LABELS, multi_label=False)
                top_topic = tres["labels"][0]
                top_topic_score = tres["scores"][0]
            except Exception as e:
                print(f"[Topic error: {e}]")
                top_topic, top_topic_score = "", 0.0
        else:
            s_label, s_score, top_topic, top_topic_score = "NEUTRAL", 0.0, "", 0.0
        decay = recency_decay(publish_dt)
        weight = (1.0 + s_score) * decay
        rows.append({
            "query": query,
            "title": title,
            "url": url,
            "snippet": item.get("snippet", ""),
            "publish_date": publish_dt.isoformat() if publish_dt else "",
            "mentions": ", ".join(mentions),
            "content_sample": page_text[:400] if isinstance(page_text, str) else "",
            "sentiment_label": s_label,
            "sentiment_score": s_score,
            "top_topic": top_topic,
            "topic_score": top_topic_score,
            "recency_decay": decay,
            "weight": weight
        })
        time.sleep(REQUEST_DELAY)
if rows:
    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
        writer.writeheader()
        writer.writerows(rows)
    print(f"\n Saved {len(rows)} unique (deduped) rows to {OUTPUT_FILE}")
else:
    print("No results found.")
relevant = [r for r in rows if r["mentions"].strip()]
counts = defaultdict(int)
weights = defaultdict(float)
pos_counts = defaultdict(int)
pos_weights = defaultdict(float)
for r in relevant:
    mentioned_brands = [b for b in brand_list if b.lower() in r["mentions"].lower()]
    for b in mentioned_brands:
        counts[b] += 1
    if mentioned_brands:
        per_brand_w = float(r["weight"]) / len(mentioned_brands)
        for b in mentioned_brands:
            weights[b] += per_brand_w
    if r["sentiment_score"] > 0:
        for b in mentioned_brands:
            pos_counts[b] += 1
            pos_weights[b] += float(r["weight"]) / len(mentioned_brands)
total_count_mentions = sum(counts.values())
total_weight = sum(weights.values())
total_pos_counts = sum(pos_counts.values())
total_pos_weights = sum(pos_weights.values())
print("\n=== Count-based SoV ===")
for b in brand_list:
    pct = (counts[b] / total_count_mentions * 100) if total_count_mentions > 0 else 0
    print(f"{b}: {counts[b]} mentions, SoV={pct:.2f}%")
print("\n=== Weighted SoV (sentiment+recency) ===")
for b in brand_list:
    pct = (weights[b] / total_weight * 100) if total_weight > 0 else 0
    print(f"{b}: weight={weights[b]:.3f}, SoV={pct:.2f}%")
print("\n=== Share of Positive Voice ===")
for b in brand_list:
    pc_pct = (pos_counts[b] / total_pos_counts * 100) if total_pos_counts > 0 else 0
    pw_pct = (pos_weights[b] / total_pos_weights * 100) if total_pos_weights > 0 else 0
    print(f"{b}: PosCount={pos_counts[b]}, CountSoV_pos={pc_pct:.2f}%, WeightedSoV_pos={pw_pct:.2f}%")
print("\n Analysis complete.")
